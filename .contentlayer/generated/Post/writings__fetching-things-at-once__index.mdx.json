{
  "title": "Fetch so many things, at once",
  "date": "2019-04-20T00:00:00.000Z",
  "published": true,
  "description": "There is the fetch API in Node, which allows us to make a HTTP request and get some information from the servers. We can use that to make REST calls, get HTML content of a webpage and many more things.",
  "tags": "node,javascript,fetch,async await",
  "banner": "hero.jpg",
  "body": {
    "raw": "\nThere is the `fetch` API in Node, which allows us to make a HTTP request and get some information from the servers. We can use that to make REST calls, get HTML content of a webpage(if we are using node for scraping) and many more things.\n\n> This article is valid for any function that returns a promise.\n\nAn example of such call goes like this\n\n```js\nfetch('/url')\n  .then((res) => res.json())\n  .then((data) => console.log(data));\n```\n\n# The Async way\n\nWe could do the same thing, using async and await.\n\n```js\nconst result = await fetch('/url');\nconst data = await result.json();\n\nconsole.log(data);\n\n// Or, a one-liner\n// const data = await (await fetch('/url')).json(); ðŸ˜‰\n```\n\n# I have so many things to fetch!\n\nOkay fine. We can do that over a classic for loop. The synchronous nature will be preserved. I mean, we can fetch one after the other, synchronously.\n\n```js\nconst urls = [...];\nfor(const url of urls) {\n    const result = await fetch(url);\n    const data = await result.json();\n\n    console.log(data);\n}\n```\n\nBut what if, the order does not matter? We can fetch them all at once. Yes, all at once, using the Promise API. After all, `fetch` returns a promise and that's why we `await` for it to be resolved.\n\nPromise API has this method `Promise.all()` , which can be awaited on for all the promises that it accepts as an argument to be resolved.\n\n```js\nconst urls = [...];\nconst promises = urls.map(url => fetch(url));\n\nawait Promise.all(promises);\n\nfor (const promise of promises) {\n    const data = await promise.json();\n    console.log(data);\n}\n```\n\nThis will save us a lot of time. Imagine we want to parse many webpages, around 100, and each webpage takes 2 seconds to be fetched and scraped for information we need. If we fetch it one after the other, it will take us around 200 seconds, which is over 3 minutes. But if we fetch all at once, it will take under a minute.\n\n# Like, really SO MANY!\n\nWhat is we have over 10000 urls to fetch. If we do the same thing as above, we will most probably not make it. We will have to face some weird socket hangup error. What can we do about it?\n\nThere is a node package called `Bluebird` which has its own Promise API and it functions the same. It has this method called `map`, which takes an extra options argument where we can set concurrency.\n\n`Promise.map(urls => fetch(url), { concurrency: 100 });`\n\nThis will, as we can infer from the line, concurrently fetch 100 requests at a time. This will save a significant load on CPU.\n\n```js\nconst Promise = require('bluebird').Promise;\nconst urls = [...];\nconst promises = await Promise.map(\n    urls => fetch(url),\n    { concurrency: 100 }\n);\n\nfor (const promise of promises) {\n    const data = await promise.json();\n    console.log(data);\n}\n```\n\nThanks for making it till the end.\n\nKeep on Hacking! âœŒ\n",
    "code": "var Component=(()=>{var d=Object.create;var s=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var w=(t,e)=>()=>(e||t((e={exports:{}}).exports,e),e.exports),g=(t,e)=>{for(var a in e)s(t,a,{get:e[a],enumerable:!0})},c=(t,e,a,r)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let o of u(e))!f.call(t,o)&&o!==a&&s(t,o,{get:()=>e[o],enumerable:!(r=m(e,o))||r.enumerable});return t};var b=(t,e,a)=>(a=t!=null?d(p(t)):{},c(e||!t||!t.__esModule?s(a,\"default\",{value:t,enumerable:!0}):a,t)),y=t=>c(s({},\"__esModule\",{value:!0}),t);var l=w((x,i)=>{i.exports=_jsx_runtime});var T={};g(T,{default:()=>P,frontmatter:()=>j});var n=b(l()),j={title:\"Fetch so many things, at once\",date:\"2019-04-20\",type:\"Post\",published:!0,description:\"There is the fetch API in Node, which allows us to make a HTTP request and get some information from the servers. We can use that to make REST calls, get HTML content of a webpage and many more things.\",tags:\"node,javascript,fetch,async await\",banner:\"hero.jpg\"};function h(t){let e=Object.assign({p:\"p\",code:\"code\",blockquote:\"blockquote\",pre:\"pre\",h1:\"h1\"},t.components);return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(e.p,{children:[\"There is the \",(0,n.jsx)(e.code,{children:\"fetch\"}),\" API in Node, which allows us to make a HTTP request and get some information from the servers. We can use that to make REST calls, get HTML content of a webpage(if we are using node for scraping) and many more things.\"]}),`\n`,(0,n.jsxs)(e.blockquote,{children:[`\n`,(0,n.jsx)(e.p,{children:\"This article is valid for any function that returns a promise.\"}),`\n`]}),`\n`,(0,n.jsx)(e.p,{children:\"An example of such call goes like this\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-js\",children:`fetch('/url')\n  .then((res) => res.json())\n  .then((data) => console.log(data));\n`})}),`\n`,(0,n.jsx)(e.h1,{children:\"The Async way\"}),`\n`,(0,n.jsx)(e.p,{children:\"We could do the same thing, using async and await.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-js\",children:`const result = await fetch('/url');\nconst data = await result.json();\n\nconsole.log(data);\n\n// Or, a one-liner\n// const data = await (await fetch('/url')).json(); \\u{1F609}\n`})}),`\n`,(0,n.jsx)(e.h1,{children:\"I have so many things to fetch!\"}),`\n`,(0,n.jsx)(e.p,{children:\"Okay fine. We can do that over a classic for loop. The synchronous nature will be preserved. I mean, we can fetch one after the other, synchronously.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-js\",children:`const urls = [...];\nfor(const url of urls) {\n    const result = await fetch(url);\n    const data = await result.json();\n\n    console.log(data);\n}\n`})}),`\n`,(0,n.jsxs)(e.p,{children:[\"But what if, the order does not matter? We can fetch them all at once. Yes, all at once, using the Promise API. After all, \",(0,n.jsx)(e.code,{children:\"fetch\"}),\" returns a promise and that's why we \",(0,n.jsx)(e.code,{children:\"await\"}),\" for it to be resolved.\"]}),`\n`,(0,n.jsxs)(e.p,{children:[\"Promise API has this method \",(0,n.jsx)(e.code,{children:\"Promise.all()\"}),\" , which can be awaited on for all the promises that it accepts as an argument to be resolved.\"]}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-js\",children:`const urls = [...];\nconst promises = urls.map(url => fetch(url));\n\nawait Promise.all(promises);\n\nfor (const promise of promises) {\n    const data = await promise.json();\n    console.log(data);\n}\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"This will save us a lot of time. Imagine we want to parse many webpages, around 100, and each webpage takes 2 seconds to be fetched and scraped for information we need. If we fetch it one after the other, it will take us around 200 seconds, which is over 3 minutes. But if we fetch all at once, it will take under a minute.\"}),`\n`,(0,n.jsx)(e.h1,{children:\"Like, really SO MANY!\"}),`\n`,(0,n.jsx)(e.p,{children:\"What is we have over 10000 urls to fetch. If we do the same thing as above, we will most probably not make it. We will have to face some weird socket hangup error. What can we do about it?\"}),`\n`,(0,n.jsxs)(e.p,{children:[\"There is a node package called \",(0,n.jsx)(e.code,{children:\"Bluebird\"}),\" which has its own Promise API and it functions the same. It has this method called \",(0,n.jsx)(e.code,{children:\"map\"}),\", which takes an extra options argument where we can set concurrency.\"]}),`\n`,(0,n.jsx)(e.p,{children:(0,n.jsx)(e.code,{children:\"Promise.map(urls => fetch(url), { concurrency: 100 });\"})}),`\n`,(0,n.jsx)(e.p,{children:\"This will, as we can infer from the line, concurrently fetch 100 requests at a time. This will save a significant load on CPU.\"}),`\n`,(0,n.jsx)(e.pre,{children:(0,n.jsx)(e.code,{className:\"language-js\",children:`const Promise = require('bluebird').Promise;\nconst urls = [...];\nconst promises = await Promise.map(\n    urls => fetch(url),\n    { concurrency: 100 }\n);\n\nfor (const promise of promises) {\n    const data = await promise.json();\n    console.log(data);\n}\n`})}),`\n`,(0,n.jsx)(e.p,{children:\"Thanks for making it till the end.\"}),`\n`,(0,n.jsx)(e.p,{children:\"Keep on Hacking! \\u270C\"})]})}function k(t={}){let{wrapper:e}=t.components||{};return e?(0,n.jsx)(e,Object.assign({},t,{children:(0,n.jsx)(h,t)})):h(t)}var P=k;return y(T);})();\n;return Component;"
  },
  "_id": "writings/fetching-things-at-once/index.mdx",
  "_raw": {
    "sourceFilePath": "writings/fetching-things-at-once/index.mdx",
    "sourceFileName": "index.mdx",
    "sourceFileDir": "writings/fetching-things-at-once",
    "contentType": "mdx",
    "flattenedPath": "writings/fetching-things-at-once"
  },
  "type": "Post",
  "slug": "/writings/fetching-things-at-once"
}